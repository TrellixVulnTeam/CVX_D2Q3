# 第 1 章  机器学习中的凸优化基础

无论做任何事情，人们总是希望以最小的代价获得最大的利益，力求最好! 为此，人们发明各式各样的数学工具：导数，积分等。 现代优化理论大都来源于处理多元问题的理论，它有三个重要的基础：

- 矩阵理论：**矩阵**是描述多元问题的最基本的工具，为多元问题分析和求解提供了基本的数据结构，同时为并行运算提供了理论支持。
- 数值分析：导数和微分为多元问题分析和求解提供了基本的数学方法和理论支持。
- 计算机与编程语言：为多元问题分析和求解提供了基本的实践工具。

由此，一个最优化问题需要我们同时具备三种基本的能力：**数学建模、公式推导、算法设计**。

本章主要涉及到的知识点有：

- 最优化是数学描述：令我们从理论角度看待优化问题。
- 凸优化理论：列举一些简单的但又十分重要的理论。
- 计算复杂性：简单的介绍了计算复杂度。

## 1.1  最优化的数学描述

一般的最优化问题都可以使用如下形式进行描述：

$$
\begin{aligned}
\min_{x\in \mathbb{R}^n}\; & f(x) & (\text{等价于  } \max_{x\in \mathbb{R}^n} -f(x))\\
\text{s.t.  } & \begin{cases}
h_i(x) = 0\\
g_j(x) \leq 0
\end{cases}
\end{aligned}
$$

- $x$ 被称为**决策变量**或问题的解
- $\text{s.t.}$ 为英文 subject to 的缩写，表示受制于，即约束条件
- $f(x)$ 称为目标函数、损失函数或代价函数 (Cost Function)
- $h(x)$ 为等式约束，$g(x)$ 为不等式约束

除此之外，最优化问题中的无约束问题也可以描述为

$$
 \arg \min_x\, f(x), (\Leftrightarrow \arg \max_x\, -f(x))
$$

其中 $\arg\max$ 符号是指求解当函数 $f(x)$ 达到最小值 (或最大值) 时 $x$ 的取值。

根据目标函数与约束函数的不同形式，可以把最优化问题分为不同的类型:

- 若 $f(x)$, $h(x)$, $g(x)$ 均为线性函数，就称为**线性规划**
- 若任意其中一个是非线性函数，则就称为**非线性规划**
- 若 $f(x)$, $h(x)$, $g(x)$ 均为凸函数，就称为**凸优化**
- 若目标函数为二次函数 (如二次型)，约束全为线性函数，就称为**二次规划**
- 若目标函数为向量函数，则称为**多目标规划**
- 其他。

下面我们正式进行凸优化的知识海洋。仅仅是罗列一些数学课本中的重要理论，为理解机器学习算法提供了理论视角。

## 1.2  凸集与分离定理

设 $A$ 是线性空间 $X$ 中的子集，$x, y \in X$，集合 $\{\lambda x + (1- \lambda y: 0 \leq \lambda \leq 1\}$ 称为联结 $x,y$ 两点的**线段**，记作 $[x,y]$。若对 $\forall x,y \in A, [x,y] \in A$，则称 $A$ 为 $X$ 中的**凸集**，而集 $\{x = \displaystyle\sum_{k=1}^n \lambda_kx_k: \lambda_k \geq 0, \displaystyle\sum_{k=1}^n \lambda_k = 1\}$ 称为 $x_1,x_2,\cdots, x_n$ 的**凸组合**。(易推知，$X$ 的线性子空间是凸集。)

设 $A_{\alpha} (\alpha \in I)$ 为凸集，则 $\underset{\alpha \in I}\cap A_{\alpha}$ 仍为凸集，设 $A_{\alpha}$ 是包含 $A$ 的所有凸集，则 $\underset{\alpha \in I}\cap A_{\alpha}$ 称为由 $A$ 生成的凸集，或集 $A$ 的**凸包**，记作 $\text{co} (A)$，它是包含 $A$ 的最小凸集。

集 $A$ 的**核**定义为

$$
A^{\circ} = \{x: \forall y \in X, \exists \delta = \delta(y)>0, \,\text{s.t. } |t| > δ, x+ty \in A \}
$$

若 $A$ 是凸集，且 $A^{\circ} \neq ∅ ,$ 则称 $A$ 为**凸体**。在赋范线性空间 $(X, ||\cdot ||)$ 中，凸体 $A$ 可定义为

若 $∀ x,y \in A, x \neq y, ||x|| = ||y||,$ 则 $||x+y|| < ||x|| + ||y||.$

### 1.2.1  凸集的几何意义

若 $x,y \in S(0, r)$，即 $||x||=||y||=r$， 则联结 $x,y$ 线段的中点 $(x+y)/2 \in B(0,r)$(球体)。即若 $x,y$ 在同一球面上，则线段 $[x,y]$ 的中点就位于该球体的内部。

### 1.2.2  超平面

设 $X$ 为实数域 $\mathbb{R}$ 上的线性空间，$f$ 是 $X$ 上的实值泛函，则

$$
L_f (\alpha) = \{x\in X: f(x) = \alpha, \alpha \in \mathbb{R} \}
$$

称为 $X$ 中的**超平面**。

### 1.2.3  支撑超平面

设 $S$ 是[拓扑向量空间](https://zh.wikipedia.org/zh-hans/拓撲向量空間)[^1] $X=\mathbb{R}^n$ 上的一个凸集，且 $x_0$ 是 $S$ 的边界上的一个点，则存在一个包含点 $x_0$ 的**支撑超平面**。若 $x^* \in X^* \backslash \{0\}$，其中 $X^*$ 是 $X$ 的对偶空间(dual space)，$x^*$ 是非零线性泛函。这样，对于所有的 $x\in S$, 有 $x^*\left(x_0\right) \geq x^*(x)$，则有

$$
H = \{x \in X: x^*(x) = x^*\left(x_0\right)\}
$$

被定义为[支撑超平面](https://en.wikipedia.org/w/index.php?title=Supporting_hyperplane)[^2] (Supporting hyperplane)。

[^1]: https://zh.wikipedia.org/zh-hans/拓撲向量空間

[^2]: https://en.wikipedia.org/w/index.php?title=Supporting_hyperplane

### 1.2.4  凸集分离定理

[凸集分离定理[^3]（Hyperplane separation theorem）](https://en.wikipedia.org/wiki/Hyperplane_separation_theorem)：有 $\mathbb{R}^n$ 中两个非空的 凸集 $A$ 和 $B$。存在非零向量 $v$ 和实数 $c$，使得

$$\langle x, v \rangle \ge c \, \text{ 和 } \langle y, v \rangle \le c$$

且 $∀ x \in A, y \in B$，则称超平面 $\langle x, \cdot \rangle = c$ 分离(disjoint) $A$ 和 $B$。

[^3]: https://en.wikipedia.org/wiki/Hyperplane_separation_theorem

凸集分离定理的意义在于：它为分类问题提供了理论上的支持。在机器学习的分类问题中，我们可以把带有类别标签的训练集看作不同的凸集，而分隔它们的超平面就是**分类器**。我们的目标是根据这些训练集的特性，找到一个分类算法，通过学习或训练并计算出这些凸集的超平面从而达到了分类的目的。

## 1.3  凸函数

凸函数是[凸集](1)中元素的数学特征表示，体现了凸集中元素所呈现的规律性。它被定义为某个向量空间的凸子集 $C$ 上的实值函数 $f$。若在其定义域 $C$ 上的任意两点 $x_1,x_2$，及 $\alpha \in [0,1]$，均有

$$
    f(\alpha x_1 +(1- \alpha)x_2) \leq \alpha f(x_1) + (1- \alpha)f(x_2)
$$

### 1.3.1  凸函数的判定

1. 设 $f_1,f_2,\cdots, f_k$ 是凸集 $S$ 上的凸函数，令 $\phi(x)= \displaystyle\sum_{i=1}^k λ_if_i(x)$，其中 $∀ λ_i \geq 0 $，则 $\psi(x)=\displaystyle\max_{1\leq i \leq k} f_i(x)$ 与 $\phi(x)$ 都是 $S$ 上的凸函数。

2. 设在凸集 $D \subset \mathbb{R}^n $ 上 $f(x)$ 可微, 则 $f(x)$ 在 $D$ 上的为凸函数的充要条件是对于任意的 $x,y\in D $, 都有

$$
    f(y) \geq f(x) + \nabla f(x)^T(y-x)
$$

3. 设在开凸集 $D \subset \mathbb{R}^n $ 上 $f(x)$ 二阶可微，则 $f(x)$ 在 $D$ 上的为凸函数的充要条件是对于任意的 $x\in D$，$f(x)$ 的 Hesse 矩阵半正定。

$$
    G(x) = ∇^2 f(x) = \begin{bmatrix}
    \frac{∂^2f}{∂x_1^2} & \cdots & \frac{∂^2f}{∂x_1x_n}\\
    \vdots & \ddots & \vdots\\
    \frac{∂^2f}{∂x_nx_1} & \cdots & \frac{∂^2f}{∂x_n^2}
    \end{bmatrix}
$$

### 1.3.2  常用的凸函数

- 线性函数和仿射函数都是凸函数
- 最大值函数
- 幂函数: 当 $\alpha\in [0,1]$ 时, $x^{\alpha}$ 是一个凸函数; 绝对值幂函数也是凸函数。
- 对数函数 $\log(x)$ 在 $\mathbb{R}_{++}$ 上是凸函数
- $f(x) = \log(\displaystyle\sum_{i=1}^n \exp(x_i))$ 是 $\mathbb{R}^n$ 上的凸函数
- 几何平均: $ f(x) = (\displaystyle∏_{i=1}^n x_i)^{\frac{1}{n}} $ 是定义在 $\mathbb{R}_{++}^n$ 上的凸函数
- 范数

### 1.3.3  凸函数的性质

- 任一局部极小 (大) 点也是全局极小 (大) 点，且全局极小 (大) 点的集合为凸集。
- 任一局部最优解都是它的整体最优解。

在最优化理论中，局部最优解被称为**满意解**，全局最优解被称为**最优解**。大多数传统的最优化理论和算法都只能保证找到满意解，因而人们尽可能的使用凸函数作为优化问题的目标函数。对于那些无法转换为凸函数的优化问题，只有通过穷举法来计算函数的所有值 (如果可能) 来找到全局最优解。当然针对一些特定的问题可以通过，诸如模拟退火法、隐马尔科夫链算法等随机优化方法来寻找最优解。但这不是我们讨论的要点，我们主要列举一些我认为比较重要的凸函数的相关知识点。

## 1.4  计算复杂性与 NP 问题

衡量算法的两个重要指：时间复杂度与空间复杂度。

如果不考虑算法的优劣，对于同一个算法执行过程所占用的资源越多，则说明该算法要解决的问题的复杂性越高。在各类算法理论中，通常将在多项式时间内即可解决的问题看作易解问题，需要指数时间才可解决的问题看作是难解问题。因而，当前算法研究领域的一个重要任务是如何将指数时间算法变换为多项式算法？

除了问题规模和运算时间的比较之外，衡量一个算法还需要考虑**确定性**和**非确定性**的概念。下面先引入**自动机模型**，或简称为**自动机** (Automata Machine) 或机 (Machine)，实际上常指一种基于状态变化进行迭代的算法 (如玻耳兹曼机 (Boltzman) 和支持向量机)。在算法领域常把这类算法看作是一个机器。

所谓确定性是指针对各种自动机模型， 根据当时的状态和输入， 若自动机的状态转移是唯一确定的，则称**确定性**；在每一时刻，若自动机有多个状态可供选择，并尝试执行每个可选择的状态时，则称为**非确定性**. 通俗地说，确定性就是程序在每个运行时 (自动机状态转移时) 产生下一步的结果是唯一的， 因此返回的结果也是唯一的。而非确定性就是在每个运行时执行路径是并行的 (随机的)：所有路径都有可能返回结果，也可能只有部分返回结果，也可能不返回结果，但只要有一个路径能够返回结果，那么算法就会结束。

在求解优化问题时，非确定性算法可能会陷入局部最优，因为所有并行的路径中某个执行路径运行达到最优，算法就会结束，但此时返回的不一定是全局最优解。

下面我们便可以定义问题的计算复杂度了。

- **$P$ 类问题**就是指能够以多项式时间的确定性算法来对问题进行判定或求解，实现它的算法在每个运行时的状态都是唯一的，最终一定能够确定唯一的结果——最优解的。
- **$NP$ 类问题**就是指可以用多项式时间的非确定性算法来对问题进行判定或求解。
    - **$NP$ 完全问题**是 NP 类问题中最难的问题，其中任何一个问题至今都没有找到多项式时间的算法。

更多参考：[P versus NP problem](https://en.wikipedia.org/wiki/P_versus_NP_problem)[^4]

关于凸优化的详细内容可参考：[图片: 凸优化基础](https://www.imooc.com/article/75121)[^5]
 和 [凸优化关键点](https://www.jianshu.com/p/dbfbdc9fb668)[^6]。

[^4]: https://en.wikipedia.org/wiki/P_versus_NP_problem

[^5]: https://www.imooc.com/article/75121

[^6]: https://www.jianshu.com/p/dbfbdc9fb668